#urlparse
from urllib.parse import urlparse,urlunparse,urljoin,urlencode
result = urlparse('http"//www.baidu.com/index.html;user?id=5#comment')
print(type(result),result)
result1 = urlparse('www.baidu.com/index.html;user?id=5#comment',scheme='https')
print(result1)
result3 = urlparse('http"//www.baidu.com/index.html;user?id=5#comment',scheme='https')
print(result3)

#锚点连接
result4 = urlparse('http"//www.baidu.com/index.html;user?id=5#comment',allow_fragments=False)
print(result4)
result5 = urlparse('http"//www.baidu.com/index.html#comment',allow_fragments=False)
print(result5)

#urlunparse 是urlparse的反函数
data=['http','www.baidu.com','index.html','user','a=6','comment']
print(urlunparse(data))

#urljion  拼接URL，后面补充
print(urljoin('http://www.baidu.com','FAQ.html'))
print(urljoin('http://www.baidu.com','https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html','https://cuiqingcai.com/FAQ.html?question=2'))

#urlencode把字典对象转成get请求参数
params={'name':'germey','age':22}
base_url='http://www,baidu.com?'
url= base_url+urlencode(params)
print(url)


#233

whj dhmdsy  sdjsdku


git merge合并时遇上refusing to merge unrelated histories的解决方案
如果git merge合并的时候出现refusing to merge unrelated histories的错误，原因是两个仓库不同而导致的，需要在后面加上--allow-unrelated-histories进行允许合并，即可解决问题




#一、大数据时代，大数据的获取方式
'''1.企业生产的用户数据：大型互联网公司海量用户，所以他们积累数据有天然的优势
        有数据意识的中小企业，也开始积累数据
  2.数据管理资询公司：麦肯锡、埃森哲
    通常这样的公司有庞大的数据采集团队，一般会通过市场调研、问卷调查、
  固定的样本检测、和各行各业公司合作、专家对话

  3.政府/官方机构提供的公开数据：国家统计局、地方政府统计、权威的第三方机构

  4.第三方数据平台购买数据：通过各个数据交易平台购买数据，根据获取的难易程度，价格不同

  5.爬虫爬取数据：市场没有或者价格太贵

二、什么是爬虫？
爬虫：抓取网页数据的程序（定向抓取）

网页三大特征：
1.每个网页都有自己唯一的URL地址(统一资源定位符)来进行定位
2.网页都使用HTML(超文本语言)来描述信息
3.网页都使用HTTP/HTTPS(超文本传输协议)协议来传输HTML数据。

三、爬虫设计思路：
1.首先确定需要爬取网页URL地址
2.通过HTTP/HTTPS协议来获取对应HTML页面
3.提取HTML里有用的数据：
    a.如果需要就保存起来
    b.如果是页面的其他URL，那就继续执行第二步
四、为什么选择Python做爬虫？
可以爬虫的语言很多，如PHP，Java、c|c++、Python等等

   PHP，虽然是世界上最好的语言，但是天生不是干这个，而且对多线程、异步支持不够好，并发处理
    能力弱，爬虫是工具性程序，对速度和效率要求比较高
   Java，的网络爬虫生态圈很完善，是Python爬虫最大的对手，但是Java语言本身很笨重，代码量大。
   重构成本高，任何修改都会导致代码大量变动，爬虫需要经常修改部分采集代码。
   c/c++ 运行效率和性能几乎最强，但是学习成本高。代码成型比较忙，只能是能力的表现，但不是
   正确的选择
   python， 语法优美、代码简洁、开发效率高、支持模块多，相关的http请求模块和HTML解析模块
   非常丰富，还有强大的爬虫scrapy爬虫框架，以及成熟高效的scrapy_redis分布式策略
   而且，调用其他接口也非常方便（胶水语言）
五、课程介绍：
1.python基本语法知识
2.如何抓取HTML页面
        http请求的处理，urllib、urllib2、requests
            处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件
3.解析服务器响应的内容
        re 正则表达式 、xpath、beautifulSoup4（bs4）、jisonpath、pyquery等
        使用某种描述性语言来给我们需要提取的数据定义一个匹配规则，符合这个规则的
        数据就会被匹配
4.如何采集动态HTML、验证码的处理
    通用的动态页面采集的工具：Selenium + PhantomJS(无界面)：模拟真实浏览器加载js、ajax等
      非静态的页面数据
    Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码，复杂的可以通过手动的
    输入或者专门的打码平台
5.Scrapy框架：（scrapy、pyspider）
    高定制性（异步网络框架twisted），所以数据下载速度非常快，提供了数据存储、数据下载、
    提取规则等组件。
6.分布式策略：
    scrapy_redis，在scrapy的基础上添加了一套以Redis 数据库为核心的组件。
    让Scrapy框架支持分布式的功能，主要在Redis里做请求指纹去重、请求分配、数据临时存储。
    python去重最主要的是（“set集合”）
7.爬虫  ...反爬虫....反反爬虫...之间的斗争
        其实python最后最头疼不是复杂的页面，也不是晦涩的数据，而是网站反爬虫师，
        User-Agent、代理、验证码、动态数据加载、加密数据
  数据的价值，是否值得去费劲做反爬虫
    1.机器成本+ 人力成本 > 数据价值，就不反了，一般封ip就结束了。
    2.面子的战争......
  爬虫和反爬虫的斗争，最后一定是爬虫获胜，
  为什么？只要是真实用户可以用浏览器获取的网页数据，爬虫一定能爬下来！

六、通用爬虫、聚焦爬虫
    1、通用爬虫：搜索引擎的爬虫系统
    1.目标：就是尽可能把互联网上所有的网页下载下来，放到本地服务器里形成备份
        在对这些网页做相关的处理（提取关键字、去掉广告），最后提供一个用户检索接口

    2.抓取流程：
        a) 首先选取一部分已有的URL，把这些URL放到待爬取的队列。
        b)从队列里取出这些URL，然后解析DNS得到主机IP，然后去ip对应的服务器里下载HTML
        页面，保存到搜索引擎的本地服务器里，之后把这个爬过的URL放入以爬取的队列。
        c) 分析网页内容，找出网页里其他的URL链接，继续执行第二步，直到爬取条件结束。
    3.搜索引擎如何获取一个新网站的URL：
        1.主动向搜索引擎提交网址：百度站长提交
        2.其他网站设置网站的外部链接。
        3.搜索引擎和DNS服务商进行合作，可以快速收录新的网站。

        DNS：就是把域名解析成IP的一种技术。

    4.通用爬虫并不是皆可爬，他也需要遵循规则：
    Robots协议：协议会指明通用爬虫可以爬取网页的权限
    Robots.txt 并不是所有爬虫都遵守，一般大型的搜索引擎爬虫才会遵守，个人写的爬虫
    就不管了。

    5.通用爬虫的工作流程：爬取网页、存储数据、内容处理。提供检索/排名服务
    6.搜索引擎排名：
        1.pageRank值：根据网站的流量（点击量、浏览量、人气）统计，流量越高，排名越靠前
        2.竞价排名，谁给钱多，谁排名就高。
    7.通用爬虫的缺点：
    1.只能提供和文本相关的内容（HTML、word、PDF）等，
    但是不能提供多媒体文件（音乐、图片、视频）和二进制文件（程序或者脚本）
    2.提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果。
    3.不能理解人类语义上的检索
为了解决通用爬虫不能解决的问题，就要聚焦爬虫
    聚焦爬虫：爬虫程序员写的针对某种内容的爬虫
    面向主题的爬虫，面向需求的爬虫：会针对某种特定的内容去爬取信息，而且会保证内容信息
    和需求尽可能相关。
七、http、https的获取方法有get或者post

  '''



nice gooo   哇喔啊


“中国最好大学排名”的排名范围是教育部公布的全国普通高等学校名单（2015年5月21日公布）中，1216所办学层次为本科的大学。这其中公办大学793所、民办大学140所、独立学院283所。
查看排名方法”

移动前端开发之viewport的深入理解
在移动设备上进行网页的重构或开发，首先得搞明白的就是移动设备上的viewport了，只有明白了viewport的概念以及弄清楚了跟viewport有关的meta标签的使用，才能更好地让我们的网页适配或响应各种不同分辨率的移动设备。

一、viewport的概念

通俗的讲，移动设备上的viewport就是设备的屏幕上能用来显示我们的网页的那一块区域，在具体一点，就是浏览器上(也可能是一个app中的webview)用来显示网页的那部分区域，但viewport又不局限于浏览器可视区域的大小，它可能比浏览器的可视区域要大，也可能比浏览器的可视区域要小。在默认情况下，一般来讲，移动设备上的viewport都是要大于浏览器可视区域的，这是因为考虑到移动设备的分辨率相对于桌面电脑来说都比较小，所以为了能在移动设备上正常显示那些传统的为桌面浏览器设计的网站，移动设备上的浏览器都会把自己默认的viewport设为980px或1024px（也可能是其它值，这个是由设备自己决定的），但带来的后果就是浏览器会出现横向滚动条，因为浏览器可视区域的宽度是比这个默认的viewport的宽度要小的。下图列出了一些设备上浏览器的默认viewport的宽度。




